{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGqhBnh2B3znlR9WlRp2bf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melodyjansen/AML-local-attention/blob/main/local_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets --quiet"
      ],
      "metadata": {
        "id": "dNGmrkOQwiWj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "-Z83gIbswysJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained BERT and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "JWb9BNptwmTG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertConfig\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertLayer, BertEncoder, BertModel\n",
        "\n",
        "# Step 1: Custom attention class\n",
        "class CustomBertSelfAttention(BertSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        # Standard BERT attention\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # Inject custom attention scores mask\n",
        "        if custom_scores_mask is not None:\n",
        "            attention_scores += custom_scores_mask\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention_mask: (batch_size, 1, 1, seq_len)\n",
        "            extended_mask = attention_mask[:, None, None, :]  # add head and query dims\n",
        "            extended_mask = (1.0 - extended_mask) * -10000.0  # 0 --> 0, 1 --> -10000\n",
        "            attention_scores = attention_scores + extended_mask\n",
        "\n",
        "\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context = context_layer.view(context_layer.size(0), -1, self.all_head_size)\n",
        "\n",
        "        return (new_context,)\n"
      ],
      "metadata": {
        "id": "EPIL0GNJyKhQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertAttention\n",
        "\n",
        "class CustomBertAttention(BertAttention):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.self = CustomBertSelfAttention(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            custom_scores_mask=custom_scores_mask,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        return attention_output\n"
      ],
      "metadata": {
        "id": "KSsjeBBByoPH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Patch model to use custom attention\n",
        "from transformers.models.bert.modeling_bert import BertLayer\n",
        "\n",
        "class CustomBertLayer(BertLayer):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.attention = CustomBertAttention(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        attention_output = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertEncoder\n",
        "\n",
        "class CustomBertEncoder(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.layer = nn.ModuleList([CustomBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            hidden_states = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                custom_scores_mask=custom_scores_mask\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "\n",
        "class CustomBertModel(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.encoder = CustomBertEncoder(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, custom_scores_mask=None):\n",
        "        embedding_output = self.embeddings(input_ids=input_ids)\n",
        "        encoder_output = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        return encoder_output\n"
      ],
      "metadata": {
        "id": "MB47x6z93cJW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train for sentiment analysis"
      ],
      "metadata": {
        "id": "NvP0XIcZsuPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CustomBertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = CustomBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, custom_scores_mask=None, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        # Grab [CLS] token (first token's output)\n",
        "        cls_output = outputs[:, 0]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return loss, logits\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "DVH4pDbNsxAv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset stuff"
      ],
      "metadata": {
        "id": "-6qk1p65nLoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-F2UX4Qp7l8",
        "outputId": "509d0522-c0c3-4cd3-beeb-346b72400185"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 13:52:45--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  17.7MB/s    in 7.8s    \n",
            "\n",
            "2025-07-06 13:52:53 (10.3 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_imdb_data(data_dir):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_type in ['pos', 'neg']:\n",
        "        dir_name = os.path.join(data_dir, label_type)\n",
        "        for fname in os.listdir(dir_name):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
        "                    texts.append(f.read())\n",
        "                labels.append(1 if label_type == 'pos' else 0)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_imdb_data('aclImdb/train')\n",
        "test_texts, test_labels = load_imdb_data('aclImdb/test')\n",
        "\n",
        "print(f'Training samples: {len(train_texts)}')\n",
        "print(f'Testing samples: {len(test_texts)}')\n",
        "print(train_texts[0], train_labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzsjeT1qF6Y",
        "outputId": "4cf2a9ec-42ac-45ce-f479-d7b0635e2a3e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 25000\n",
            "Testing samples: 25000\n",
            "There are enough sad stories about women and their oppression by religious, political and societal means. Not to diminish the films and stories about genital mutilation and reproductive rights, as well as wage inequality, and marginalization in society, all in the name of Allah or God or some other ridiculous justification, but sometimes it is helpful to just take another approach and shed some light on the subject.<br /><br />The setting is the 2006 match between Iran and Bahrain to qualify for the World Cup. Passions are high and several women try to disguise themselves as men to get into the match.<br /><br />The women who were caught (Played by Sima Mobarak-Shahi, Shayesteh Irani, Ayda Sadeqi, Golnaz Farmani, and Mahnaz Zabihi) and detained for prosecution provided a funny and illuminating glimpse into the customs of this country and, most likely, all Muslim countries. Their interaction with the Iranian soldiers who were guarding and transporting them, both city and villagers, and the father who was looking for his daughter provided some hilarious moments as we thought about why they have such unwritten rules.<br /><br />It is mainly about a paternalistic society that feels it has to save it's women from the crude behavior of it's men. Rather than educating the male population, they deny privilege and rights to the women.<br /><br />Seeing the changes in the soldiers responsible and the reflection of Iranian society, it is nos surprise this film will not get any play in Iran. But Jafar Panahi has a winner on his hands for those able to see it. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def encode_texts(texts, labels, tokenizer, max_length=64):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "    input_ids = torch.tensor(encodings['input_ids'])\n",
        "    attention_mask = torch.tensor(encodings['attention_mask'])\n",
        "    labels = torch.tensor(labels)\n",
        "    return TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Shuffle before taking subset\n",
        "train_texts, train_labels = shuffle(train_texts, train_labels, random_state=42)\n",
        "test_texts, test_labels = shuffle(test_texts, test_labels, random_state=42)\n",
        "\n",
        "# Subset\n",
        "train_texts = train_texts[:1000]\n",
        "train_labels = train_labels[:1000]\n",
        "test_texts = test_texts[:200]\n",
        "test_labels = test_labels[:200]\n",
        "\n",
        "train_dataset = encode_texts(train_texts, train_labels, tokenizer)\n",
        "test_dataset = encode_texts(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n"
      ],
      "metadata": {
        "id": "9Ed6OTmCrBtr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_attention_mask(seq_len, window_size, heads):\n",
        "    mask = torch.full((1, heads, seq_len, seq_len), float('-inf'))\n",
        "    for i in range(seq_len):\n",
        "        mask[0, :, i, max(0, i-window_size):i+window_size+1] = 0\n",
        "    return mask"
      ],
      "metadata": {
        "id": "WYmPV4qlrEpz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# instantiate custom model with local attention\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "model = CustomBertForSequenceClassification(config, num_labels=2)  # binary classification\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "epochs = 3\n",
        "batch_size = 8\n",
        "window_size = 3  # for local attention\n",
        "num_heads = 12\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader)):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        # Generate local attention mask per batch\n",
        "        seq_len = input_ids.size(1)\n",
        "        custom_scores_mask = create_local_attention_mask(seq_len, window_size, num_heads).to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        loss, logits = outputs\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1} finished. Average loss: {avg_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA30mmlQtBeF",
        "outputId": "20ed0f73-7dfc-4e2c-c287-d681314d2e9e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [12:43<00:00,  6.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished. Average loss: 0.7759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [12:25<00:00,  5.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished. Average loss: 0.7318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [12:15<00:00,  5.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 finished. Average loss: 0.7074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        seq_len = input_ids.size(1)\n",
        "        custom_scores_mask = create_local_attention_mask(seq_len, window_size, num_heads).to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv1SOLQa81MS",
        "outputId": "17f71296-229d-49b5-ff88-2349704c1275"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:42<00:00,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        95\n",
            "    Positive       0.53      1.00      0.69       105\n",
            "\n",
            "    accuracy                           0.53       200\n",
            "   macro avg       0.26      0.50      0.34       200\n",
            "weighted avg       0.28      0.53      0.36       200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  0  95]\n",
            " [  0 105]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels:\", np.unique(all_labels))\n",
        "print(\"Unique predictions:\", np.unique(all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltkJZuzw9J-9",
        "outputId": "95432c66-1621-4ff3-c0cb-a77ccca48fe5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: [0 1]\n",
            "Unique predictions: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train labels distribution:\", np.bincount(train_labels))\n",
        "print(\"Test labels distribution:\", np.bincount(test_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtEpFjx_J-yn",
        "outputId": "3cb2b475-b9e4-4ff6-b98f-19d3cf73543e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels distribution: [489 511]\n",
            "Test labels distribution: [ 95 105]\n"
          ]
        }
      ]
    }
  ]
}