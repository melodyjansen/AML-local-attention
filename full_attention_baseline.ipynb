{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3vlQLc7nszu2UtLduciO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melodyjansen/AML-local-attention/blob/main/full_attention_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets --quiet"
      ],
      "metadata": {
        "id": "dNGmrkOQwiWj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "-Z83gIbswysJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained BERT and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "JWb9BNptwmTG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertConfig\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertLayer, BertEncoder, BertModel\n",
        "\n",
        "# Step 1: Custom attention class\n",
        "class CustomBertSelfAttention(BertSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        # Standard BERT attention\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # Inject custom attention scores mask\n",
        "        if custom_scores_mask is not None:\n",
        "            attention_scores += custom_scores_mask\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention_mask: (batch_size, 1, 1, seq_len)\n",
        "            extended_mask = attention_mask[:, None, None, :]  # add head and query dims\n",
        "            extended_mask = (1.0 - extended_mask) * -10000.0  # 0 --> 0, 1 --> -10000\n",
        "            attention_scores = attention_scores + extended_mask\n",
        "\n",
        "\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context = context_layer.view(context_layer.size(0), -1, self.all_head_size)\n",
        "\n",
        "        return (new_context,)\n"
      ],
      "metadata": {
        "id": "EPIL0GNJyKhQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertAttention\n",
        "\n",
        "class CustomBertAttention(BertAttention):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.self = CustomBertSelfAttention(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            custom_scores_mask=custom_scores_mask,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        return attention_output\n"
      ],
      "metadata": {
        "id": "KSsjeBBByoPH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Patch model to use custom attention\n",
        "from transformers.models.bert.modeling_bert import BertLayer\n",
        "\n",
        "class CustomBertLayer(BertLayer):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.attention = CustomBertAttention(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        attention_output = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertEncoder\n",
        "\n",
        "class CustomBertEncoder(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.layer = nn.ModuleList([CustomBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        custom_scores_mask=None,\n",
        "    ):\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            hidden_states = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                custom_scores_mask=custom_scores_mask\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "\n",
        "class CustomBertModel(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.encoder = CustomBertEncoder(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, custom_scores_mask=None):\n",
        "        embedding_output = self.embeddings(input_ids=input_ids)\n",
        "        encoder_output = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        return encoder_output\n"
      ],
      "metadata": {
        "id": "MB47x6z93cJW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train for sentiment analysis"
      ],
      "metadata": {
        "id": "NvP0XIcZsuPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CustomBertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = CustomBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, custom_scores_mask=None, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            custom_scores_mask=custom_scores_mask\n",
        "        )\n",
        "        # Grab [CLS] token (first token's output)\n",
        "        cls_output = outputs[:, 0]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return loss, logits\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "DVH4pDbNsxAv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset stuff"
      ],
      "metadata": {
        "id": "-6qk1p65nLoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-F2UX4Qp7l8",
        "outputId": "95febcc8-6383-4417-a093-cc6a2d83f74e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 13:53:56--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  11.2MB/s    in 11s     \n",
            "\n",
            "2025-07-06 13:54:07 (7.63 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_imdb_data(data_dir):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_type in ['pos', 'neg']:\n",
        "        dir_name = os.path.join(data_dir, label_type)\n",
        "        for fname in os.listdir(dir_name):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
        "                    texts.append(f.read())\n",
        "                labels.append(1 if label_type == 'pos' else 0)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_imdb_data('aclImdb/train')\n",
        "test_texts, test_labels = load_imdb_data('aclImdb/test')\n",
        "\n",
        "print(f'Training samples: {len(train_texts)}')\n",
        "print(f'Testing samples: {len(test_texts)}')\n",
        "print(train_texts[0], train_labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzsjeT1qF6Y",
        "outputId": "e05bf957-2193-4328-fc31-6efc7450a898"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 25000\n",
            "Testing samples: 25000\n",
            "There are enough sad stories about women and their oppression by religious, political and societal means. Not to diminish the films and stories about genital mutilation and reproductive rights, as well as wage inequality, and marginalization in society, all in the name of Allah or God or some other ridiculous justification, but sometimes it is helpful to just take another approach and shed some light on the subject.<br /><br />The setting is the 2006 match between Iran and Bahrain to qualify for the World Cup. Passions are high and several women try to disguise themselves as men to get into the match.<br /><br />The women who were caught (Played by Sima Mobarak-Shahi, Shayesteh Irani, Ayda Sadeqi, Golnaz Farmani, and Mahnaz Zabihi) and detained for prosecution provided a funny and illuminating glimpse into the customs of this country and, most likely, all Muslim countries. Their interaction with the Iranian soldiers who were guarding and transporting them, both city and villagers, and the father who was looking for his daughter provided some hilarious moments as we thought about why they have such unwritten rules.<br /><br />It is mainly about a paternalistic society that feels it has to save it's women from the crude behavior of it's men. Rather than educating the male population, they deny privilege and rights to the women.<br /><br />Seeing the changes in the soldiers responsible and the reflection of Iranian society, it is nos surprise this film will not get any play in Iran. But Jafar Panahi has a winner on his hands for those able to see it. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def encode_texts(texts, labels, tokenizer, max_length=64):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "    input_ids = torch.tensor(encodings['input_ids'])\n",
        "    attention_mask = torch.tensor(encodings['attention_mask'])\n",
        "    labels = torch.tensor(labels)\n",
        "    return TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Shuffle before taking subset\n",
        "train_texts, train_labels = shuffle(train_texts, train_labels, random_state=42)\n",
        "test_texts, test_labels = shuffle(test_texts, test_labels, random_state=42)\n",
        "\n",
        "# Subset\n",
        "train_texts = train_texts[:1000]\n",
        "train_labels = train_labels[:1000]\n",
        "test_texts = test_texts[:200]\n",
        "test_labels = test_labels[:200]\n",
        "\n",
        "train_dataset = encode_texts(train_texts, train_labels, tokenizer)\n",
        "test_dataset = encode_texts(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n"
      ],
      "metadata": {
        "id": "9Ed6OTmCrBtr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_attention_mask(seq_len, window_size, heads):\n",
        "    mask = torch.full((1, heads, seq_len, seq_len), float('-inf'))\n",
        "    for i in range(seq_len):\n",
        "        mask[0, :, i, max(0, i-window_size):i+window_size+1] = 0\n",
        "    return mask"
      ],
      "metadata": {
        "id": "WYmPV4qlrEpz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cDpysvq1skRZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "\n",
        "# Init model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader)):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA30mmlQtBeF",
        "outputId": "ac051b7c-4051-4f1f-b979-bdfef346402e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 125/125 [14:36<00:00,  7.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished. Avg loss: 0.5925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [13:52<00:00,  6.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished. Avg loss: 0.3251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [13:55<00:00,  6.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 finished. Avg loss: 0.1162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits  # for HuggingFace models\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Show a few predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"True: {all_labels[i]}, Predicted: {all_preds[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH7dwPEqK5r8",
        "outputId": "4235c11e-2441-426d-edae-a58c2ec3cff0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:47<00:00,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.81      0.80        95\n",
            "    Positive       0.83      0.81      0.82       105\n",
            "\n",
            "    accuracy                           0.81       200\n",
            "   macro avg       0.81      0.81      0.81       200\n",
            "weighted avg       0.81      0.81      0.81       200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[77 18]\n",
            " [20 85]]\n",
            "\n",
            "Sample Predictions:\n",
            "True: 1, Predicted: 1\n",
            "True: 0, Predicted: 0\n",
            "True: 1, Predicted: 1\n",
            "True: 0, Predicted: 0\n",
            "True: 0, Predicted: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}